{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7812b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweetï¿½</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweetï¿½</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186342</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Madelinedugganx</td>\n",
       "      <td>My GrandMa is making Dinenr with my Mum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186409</td>\n",
       "      <td>Fri May 29 07:33:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OffRoad_Dude</td>\n",
       "      <td>Mid-morning snack time... A bowl of cheese noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186429</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Falchion</td>\n",
       "      <td>@ShaDeLa same here  say it like from the Termi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186445</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jonasobsessedx</td>\n",
       "      <td>@DestinyHope92 im great thaanks  wbuu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186607</td>\n",
       "      <td>Fri May 29 07:33:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sugababez</td>\n",
       "      <td>cant wait til her date this weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity of tweetï¿½  id of the tweet             date of the tweet  \\\n",
       "1048567                   4       1960186342  Fri May 29 07:33:44 PDT 2009   \n",
       "1048568                   4       1960186409  Fri May 29 07:33:43 PDT 2009   \n",
       "1048569                   4       1960186429  Fri May 29 07:33:44 PDT 2009   \n",
       "1048570                   4       1960186445  Fri May 29 07:33:44 PDT 2009   \n",
       "1048571                   4       1960186607  Fri May 29 07:33:45 PDT 2009   \n",
       "\n",
       "            query             user  \\\n",
       "1048567  NO_QUERY  Madelinedugganx   \n",
       "1048568  NO_QUERY     OffRoad_Dude   \n",
       "1048569  NO_QUERY         Falchion   \n",
       "1048570  NO_QUERY   jonasobsessedx   \n",
       "1048571  NO_QUERY        sugababez   \n",
       "\n",
       "                                        text of the tweetï¿½  \n",
       "1048567           My GrandMa is making Dinenr with my Mum   \n",
       "1048568  Mid-morning snack time... A bowl of cheese noo...  \n",
       "1048569  @ShaDeLa same here  say it like from the Termi...  \n",
       "1048570             @DestinyHope92 im great thaanks  wbuu?  \n",
       "1048571               cant wait til her date this weekend   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df=pd.read_csv('/Users/rishilboddula/Desktop/MLOPS/Sentiment-Analysis-MLOPS/archive (13)/training.1600000.processed.noemoticon.csv')\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3519035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishilboddula/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nBag of words- It means count how many times each word appears in a document.\\nex-Example:\\n\\nDocument\\tText\\nD1\\t\"I love dogs\"\\nD2\\t\"I love cats\"\\n\\nVocabulary = [\"I\", \"love\", \"dogs\", \"cats\"]\\n\\nDoc\\tI\\tlove\\tdogs\\tcats\\nD1\\t1\\t1\\t1\\t0\\nD2\\t1\\t1\\t0\\t1\\n\\nSo each sentence becomes a vector of word counts.\\n\\nðŸ”¸ Problem: common words (\"I\", \"love\") appear everywhere â€” not very informative.\\n\\nTFIDF-TF-IDF (Term Frequencyâ€“Inverse Document Frequency)\\nmake ccommon words less important and rare words important\\nIDF=log(N/(1+n(t)))\\nwhere N = total docs and n(t)=number of docs containing the term\\nit gives more weight to unique words\\nBut doesnt cqpture synonyms.\\nWe use this in the project because naive bayes and logistic regression do not work properly with word embeddings.\\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\nfit_transform()- learns vocalblury and IDF weights and then it converts into TF-IDF vectors\\ntransform()-Uses the same vocalblury and TF-IDF vectors from trrainig data aND just transforms the testing data to TF-IDF vectors \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import Tuple\n",
    "from typing_extensions import Annotated\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "Now in the below steps for feautre engineering we do the following\n",
    "           1-We put the text to lower\n",
    "           2- remove the user accounts in text like @elon_musk or @account\n",
    "           3-we remove any urls\n",
    "           4- we remove all puncuation marks and numbers\n",
    "           5-we put a sentence in an array, like ex-\n",
    "           i/p-I love Ferrari \n",
    "           o/p-['I','love','Ferrari;]\n",
    "           6- we remove all the the common words and convert most words to root words\n",
    "           7- WE can use WordNetLemmatizer() instead of port stemmer and that is efficient because it\n",
    "           converts the word to theier dictionary roots where as POrtstemmer uses crude rules.But WordNetLemmatizer() is\n",
    "           very slow.For this project we use Portstemmer\n",
    "           ex-Word   Stemming  Lemmatization\n",
    "            â€œstudiesâ€  studi âŒ study âœ…\n",
    "             â€œbetterâ€  better âŒ    good âœ…\n",
    "           ex o/p-\n",
    "                 0          upset cant updat facebook text might cri resul...\n",
    "                 1               dive mani time ball manag save rest go bound\n",
    "                 2                            whole bodi feel itchi like fire\n",
    "                 3                                      behav im mad cant see\n",
    "                 4                                                 whole crew\n",
    "                                ...                       \n",
    "                 1048567                              grandma make dinenr mum\n",
    "                 1048568              midmorn snack time bowl chees noodl yum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))#contains all common words in english\n",
    "stemmer=PorterStemmer()\n",
    "class Pre_Process_Strategies:\n",
    "    def handle_data(self,df:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            #nltk.download('stopwords')\n",
    "           # stop_words=set(stopwords.words('english'))\n",
    "           # stemmer=PorterStemmer()\n",
    "            df.drop(columns='id of the tweet',inplace=True)\n",
    "            df.drop(columns=['query','user'],inplace=True)\n",
    "            df.rename(columns={'polarity of tweetï¿½': 'sentiment','date of the tweet': 'date','text of the tweetï¿½': 'text'},inplace=True)\n",
    "            '''\n",
    "            It is to be noted that there are no null values and the main col is 'sentimennt' and 'text' so no other \n",
    "            '''\n",
    "            \n",
    "            df.drop(columns='date',inplace=True) \n",
    "           \n",
    "            df['text']=df['text'].str.lower()\n",
    "            df['text']=df['text'].str.replace(r'@\\w+', '', regex=True)\n",
    "            def pre_process_further(text):\n",
    "                text=re.sub(r'http\\S+|www\\S+','',text)\n",
    "                text=re.sub(r'[^a-z\\s]','',text)\n",
    "                words=text.split()\n",
    "                words=[stemmer.stem(w) for w in words if  w not in stop_words]#stemmer.stem(w) for w in words if w not in stop_words\n",
    "                return ' '.join(words)\n",
    "            df['text']=df['text'].apply(pre_process_further)\n",
    "            data=df\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while cleaning data: {e}\")\n",
    "            raise e  \n",
    "   \n",
    "    def split_test_train_and_feature_engineer(self,data:pd.DataFrame)->Tuple[\n",
    "    Annotated[np.ndarray, \"X_train\"],\n",
    "    Annotated[np.ndarray, \"X_test\"],#should be np.ndarray not no.array\n",
    "    Annotated[pd.Series, \"Y_train\"],\n",
    "    Annotated[pd.Series, \"Y_test\"]\n",
    "]:\n",
    "        try:\n",
    "            X=data.drop(columns=['sentiment'])\n",
    "            Y= data['sentiment']\n",
    "            X_train_df, X_test_df, Y_train, Y_test =train_test_split(X, Y, test_size=0.2, random_state=2)#here 0.2 means 20% of taat will be for test.this methhod is used ot split the data into train and test\n",
    "            tfid=TfidfVectorizer(max_features=5000,ngram_range=(1,2))#max_features means it tells tfidf to only consider top 5000 words, n_grams=(1,2)means it tells it t conasider eeither single words like good,bad or 2 words like not good,nnot bad etc\n",
    "            X_train=tfid.fit_transform(X_train_df['text']).toarray()\n",
    "            X_test=tfid.transform(X_test_df['text']).toarray()\n",
    "            # create a new filepath and open 'tfidf_vectorizer.pkl' 'wb' meams write in binary mode\n",
    "            with open('tfidf_vectorizer.pkl','wb') as f:\n",
    "                pickle.dump(tfid,f)#writes tfid to the file\n",
    "                print(\"Saved successfully!\")\n",
    "                           \n",
    "            return X_train, X_test, Y_train, Y_test\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while dividing data or feature engineering of data: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def clean_deployment_text(self,text:str)->np.ndarray:\n",
    "        try:\n",
    "           \n",
    "            text = text.lower()\n",
    "            text=re.sub(r'http\\S+|www\\S+','',text)\n",
    "            text=re.sub(r'[^a-z\\s]','',text)\n",
    "            text=re.sub(r'@\\w+','',text)\n",
    "            #rb=read binary\n",
    "            with open('tfidf_vectorizer.pkl','rb')as f:\n",
    "                tfid=pickle.load(f)\n",
    "            words=text.split()#splits the string on whitespace by default.\n",
    "            words=[stemmer.stem(w) for w in words if w not in stop_words]\n",
    "            cleaned_text = \" \".join(words)\n",
    "            \n",
    "            pre_processed_words = tfid.transform([cleaned_text]).toarray()\n",
    "            return pre_processed_words\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error in cleaning the raw text for deployment')\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Bag of words- It means count how many times each word appears in a document.\n",
    "ex-Example:\n",
    "\n",
    "Document\tText\n",
    "D1\t\"I love dogs\"\n",
    "D2\t\"I love cats\"\n",
    "\n",
    "Vocabulary = [\"I\", \"love\", \"dogs\", \"cats\"]\n",
    "\n",
    "Doc\tI\tlove\tdogs\tcats\n",
    "D1\t1\t1\t1\t0\n",
    "D2\t1\t1\t0\t1\n",
    "\n",
    "So each sentence becomes a vector of word counts.\n",
    "\n",
    "ðŸ”¸ Problem: common words (\"I\", \"love\") appear everywhere â€” not very informative.\n",
    "\n",
    "TFIDF-TF-IDF (Term Frequencyâ€“Inverse Document Frequency)\n",
    "make ccommon words less important and rare words important\n",
    "IDF=log(N/(1+n(t)))\n",
    "where N = total docs and n(t)=number of docs containing the term\n",
    "it gives more weight to unique words\n",
    "But doesnt cqpture synonyms.\n",
    "We use this in the project because naive bayes and logistic regression do not work properly with word embeddings.\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "fit_transform()- learns vocalblury and IDF weights and then it converts into TF-IDF vectors\n",
    "transform()-Uses the same vocalblury and TF-IDF vectors from trrainig data aND just transforms the testing data to TF-IDF vectors \n",
    "''' \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27dda7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> Tuple[\n",
    "    Annotated[np.ndarray, \"X_train\"],\n",
    "    Annotated[np.ndarray, \"X_test\"],\n",
    "    Annotated[pd.Series, \"Y_train\"],\n",
    "    Annotated[pd.Series, \"Y_test\"]\n",
    "]:\n",
    "    try:\n",
    "        processor=Pre_Process_Strategies()\n",
    "        cleaned_df=processor.handle_data(df)\n",
    "        X_train,X_test,Y_train,Y_test=processor.split_test_train_and_feature_engineer(cleaned_df)\n",
    "        logging.info(\"Data cleaning,feature_engineering and division successful\")\n",
    "        return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "\n",
    "        logging.error(f\"Error while cleaning data: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a92d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n",
      "\u001b[1;35mData cleaning,feature_engineering and division successful\u001b[0m\n",
      "\u001b[1;35mData cleaning,feature_engineering and division successful\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4425e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209715, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b229577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I really love this product! It works perfectly.\"\n",
    "cleaning_strategies=Pre_Process_Strategies()\n",
    "cleaned_words=cleaning_strategies.clean_deployment_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea86ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3903fc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f5618543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048572 entries, 0 to 1048571\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count    Dtype \n",
      "---  ------              --------------    ----- \n",
      " 0   polarity of tweetï¿½  1048572 non-null  int64 \n",
      " 1   id of the tweet     1048572 non-null  int64 \n",
      " 2   date of the tweet   1048572 non-null  object\n",
      " 3   query               1048572 non-null  object\n",
      " 4   user                1048572 non-null  object\n",
      " 5   text of the tweetï¿½  1048572 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 48.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "79f7ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='id of the tweet',inplace=True)\n",
    "df.drop(columns=['query','user'],inplace=True)\n",
    "df.rename(columns={\n",
    "    'polarity of tweetï¿½': 'sentiment',\n",
    "    'date of the tweet': 'date',\n",
    "    'text of the tweetï¿½': 'text'\n",
    "},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39c55afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048572 entries, 0 to 1048571\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1048572 non-null  int64 \n",
      " 1   date       1048572 non-null  object\n",
      " 2   text       1048572 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 24.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba4b0a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn the above line we are telling to replace all 4's with 1. For example if in a dataset there are positive,negativeand we want to replace with ones and 0s,\\nwe can do like df['col_name']=df['col_nale'].replace({positive:1,negative:0})\\n\\n\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df['sentiment']=df['sentiment'].replace({4:1})\n",
    "'''\n",
    "In the above line we are telling to replace all 4's with 1. For example if in a dataset there are positive,negativeand we want to replace with ones and 0s,\n",
    "we can do like df['col_name']=df['col_nale'].replace({positive:1,negative:0})\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be2b646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishilboddula/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "df['date']=pd.to_datetime(df['date'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a989c584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2009-04-06 22:19:49\n",
       "1   2009-04-06 22:19:53\n",
       "2   2009-04-06 22:19:57\n",
       "3   2009-04-06 22:19:57\n",
       "4   2009-04-06 22:20:00\n",
       "5   2009-04-06 22:20:03\n",
       "6   2009-04-06 22:20:03\n",
       "7   2009-04-06 22:20:17\n",
       "8   2009-04-06 22:20:19\n",
       "9   2009-04-06 22:20:19\n",
       "Name: date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "425f83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.groupby(df['date'].dt.date)['sentiment'].mean().reset_index()\n",
    "df_daily.rename(columns={'date':'Date', 'sentiment':'Avg_Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "986b2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily=df.groupby(df['date'].dt.date)['sentiment'].mean().reset_index()\n",
    "#.dt.date extracts only the date (YYYY-MM-DD) from a datetime column, removing the time.\n",
    "df_daily.rename(columns={'date':'Date','sentiment':'Avg_sentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66573821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Avg_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>0.576281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-07</td>\n",
       "      <td>0.587026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-17</td>\n",
       "      <td>0.596628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-18</td>\n",
       "      <td>0.584444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-04-19</td>\n",
       "      <td>0.584004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-04-20</td>\n",
       "      <td>0.576544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-04-21</td>\n",
       "      <td>0.591941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-05-01</td>\n",
       "      <td>0.578279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-05-02</td>\n",
       "      <td>0.584530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>0.581722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-05-04</td>\n",
       "      <td>0.584135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-05-09</td>\n",
       "      <td>0.574325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-05-10</td>\n",
       "      <td>0.588843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009-05-11</td>\n",
       "      <td>0.578356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009-05-13</td>\n",
       "      <td>0.563207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009-05-14</td>\n",
       "      <td>0.581443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2009-05-16</td>\n",
       "      <td>0.580363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>0.644145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>0.633716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2009-05-21</td>\n",
       "      <td>0.630394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2009-05-22</td>\n",
       "      <td>0.643062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2009-05-23</td>\n",
       "      <td>0.562130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>0.579882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009-05-26</td>\n",
       "      <td>0.595751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2009-05-27</td>\n",
       "      <td>0.563615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2009-05-28</td>\n",
       "      <td>0.611017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2009-05-29</td>\n",
       "      <td>0.159071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009-05-30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2009-05-31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2009-06-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2009-06-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2009-06-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2009-06-04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2009-06-06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2009-06-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2009-06-14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2009-06-15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2009-06-17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2009-06-18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2009-06-20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2009-06-21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2009-06-22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2009-06-23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2009-06-24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2009-06-25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Avg_sentiment\n",
       "0   2009-04-06       0.576281\n",
       "1   2009-04-07       0.587026\n",
       "2   2009-04-17       0.596628\n",
       "3   2009-04-18       0.584444\n",
       "4   2009-04-19       0.584004\n",
       "5   2009-04-20       0.576544\n",
       "6   2009-04-21       0.591941\n",
       "7   2009-05-01       0.578279\n",
       "8   2009-05-02       0.584530\n",
       "9   2009-05-03       0.581722\n",
       "10  2009-05-04       0.584135\n",
       "11  2009-05-09       0.574325\n",
       "12  2009-05-10       0.588843\n",
       "13  2009-05-11       0.578356\n",
       "14  2009-05-13       0.563207\n",
       "15  2009-05-14       0.581443\n",
       "16  2009-05-16       0.580363\n",
       "17  2009-05-17       0.644145\n",
       "18  2009-05-18       0.633716\n",
       "19  2009-05-21       0.630394\n",
       "20  2009-05-22       0.643062\n",
       "21  2009-05-23       0.562130\n",
       "22  2009-05-25       0.579882\n",
       "23  2009-05-26       0.595751\n",
       "24  2009-05-27       0.563615\n",
       "25  2009-05-28       0.611017\n",
       "26  2009-05-29       0.159071\n",
       "27  2009-05-30       0.000000\n",
       "28  2009-05-31       0.000000\n",
       "29  2009-06-01       0.000000\n",
       "30  2009-06-02       0.000000\n",
       "31  2009-06-03       0.000000\n",
       "32  2009-06-04       0.000000\n",
       "33  2009-06-05       0.000000\n",
       "34  2009-06-06       0.000000\n",
       "35  2009-06-07       0.000000\n",
       "36  2009-06-14       0.000000\n",
       "37  2009-06-15       0.000000\n",
       "38  2009-06-16       0.000000\n",
       "39  2009-06-17       0.000000\n",
       "40  2009-06-18       0.000000\n",
       "41  2009-06-19       0.000000\n",
       "42  2009-06-20       0.000000\n",
       "43  2009-06-21       0.000000\n",
       "44  2009-06-22       0.000000\n",
       "45  2009-06-23       0.000000\n",
       "46  2009-06-24       0.000000\n",
       "47  2009-06-25       0.000000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23610393",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `Avg_Sentiment` for `y`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAvg_Sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_daily\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Sentiment (0 = Negative, 1 = Positive)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/relational.py:485\u001b[0m, in \u001b[0;36mlineplot\u001b[0;34m(data, x, y, hue, size, style, units, weights, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlineplot\u001b[39m(\n\u001b[1;32m    472\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    473\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Handle deprecation of ci parameter\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     errorbar \u001b[38;5;241m=\u001b[39m _deprecate_ci(errorbar, ci)\n\u001b[0;32m--> 485\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_LinePlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrorbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[1;32m    496\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/relational.py:216\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[0;34m(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    204\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    213\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.linewidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    214\u001b[0m     )\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorbar \u001b[38;5;241m=\u001b[39m errorbar\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m~/Desktop/MLOPS/Sentiment-Analysis-MLOPS/myenv/lib/python3.8/site-packages/seaborn/_core/data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn entry with this name does not appear in `data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `Avg_Sentiment` for `y`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x='Date', y='Avg_Sentiment', data=df_daily)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment (0 = Negative, 1 = Positive)')\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.xticks(rotation=45)  # rotate dates to see them clearly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5a488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                date  \\\n",
       "0          0 2009-04-06 22:19:49   \n",
       "1          0 2009-04-06 22:19:53   \n",
       "2          0 2009-04-06 22:19:57   \n",
       "3          0 2009-04-06 22:19:57   \n",
       "4          0 2009-04-06 22:20:00   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his facebook by ...  \n",
       "1  @kenichan i dived many times for the ball. man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @kwesidei not the whole crew   "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.replace(r'@\\w+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ecbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>i dived many times for the ball. managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                date  \\\n",
       "0          0 2009-04-06 22:19:49   \n",
       "1          0 2009-04-06 22:19:53   \n",
       "2          0 2009-04-06 22:19:57   \n",
       "3          0 2009-04-06 22:19:57   \n",
       "4          0 2009-04-06 22:20:00   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his facebook by ...  \n",
       "1   i dived many times for the ball. managed to s...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3   no, it's not behaving at all. i'm mad. why am...  \n",
       "4                                not the whole crew   "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20a71404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                date  \\\n",
       "0          0 2009-04-06 22:19:49   \n",
       "1          0 2009-04-06 22:19:53   \n",
       "2          0 2009-04-06 22:19:57   \n",
       "3          0 2009-04-06 22:19:57   \n",
       "4          0 2009-04-06 22:20:00   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b51266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'date', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f116c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de5ed12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishilboddula/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rishilboddula/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stemmer=PorterStemmer()\n",
    "lemmetizer=WordNetLemmatizer()\n",
    "def pre_prrocess(text):\n",
    "    text=re.sub(r'http\\S+|www\\S+','',text)#remove the urls\n",
    "    text=re.sub(r'[^a-z\\s]','',text)\n",
    "    words=text.split()#puts a sentence of words in a list\n",
    "    words=[lemmetizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(pre_prrocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e5f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          upset cant updat facebook text might cri resul...\n",
       "1               dive mani time ball manag save rest go bound\n",
       "2                            whole bodi feel itchi like fire\n",
       "3                                      behav im mad cant see\n",
       "4                                                 whole crew\n",
       "                                 ...                        \n",
       "1048567                              grandma make dinenr mum\n",
       "1048568              midmorn snack time bowl chees noodl yum\n",
       "1048569                say like terminus movi come like word\n",
       "1048570                                 im great thaank wbuu\n",
       "1048571                           cant wait til date weekend\n",
       "Name: text, Length: 1048572, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da4888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>behav im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>1</td>\n",
       "      <td>grandma make dinenr mum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>1</td>\n",
       "      <td>midmorn snack time bowl chees noodl yum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>1</td>\n",
       "      <td>say like termini movi come like word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>1</td>\n",
       "      <td>im great thaank wbuu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>1</td>\n",
       "      <td>cant wait til date weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048572 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "0                0  upset cant updat facebook text might cri resul...\n",
       "1                0       dive mani time ball manag save rest go bound\n",
       "2                0                    whole bodi feel itchi like fire\n",
       "3                0                              behav im mad cant see\n",
       "4                0                                         whole crew\n",
       "...            ...                                                ...\n",
       "1048567          1                            grandma make dinenr mum\n",
       "1048568          1            midmorn snack time bowl chees noodl yum\n",
       "1048569          1               say like termini movi come like word\n",
       "1048570          1                               im great thaank wbuu\n",
       "1048571          1                         cant wait til date weekend\n",
       "\n",
       "[1048572 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.8.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
